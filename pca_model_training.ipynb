import numpy as np
import pandas as pd
import xgboost as xgb
from sklearn import preprocessing
from sklearn.decomposition import PCA
from sklearn.metrics import r2_score
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler

# Assuming train_df is already loaded
# Step 1: Label Encoding for categorical features
for f in ["X0", "X1", "X2", "X3", "X4", "X5", "X6", "X8"]:
    lbl = preprocessing.LabelEncoder()
    lbl.fit(list(train_df[f].values)) 
    train_df[f] = lbl.transform(list(train_df[f].values))

# Step 2: Prepare your features and target
train_y = train_df['y'].values
train_X = train_df.drop(["ID", "y", "eval_set"], axis=1)

# Step 3: Standardize the features (important for PCA)
scaler = StandardScaler()
train_X_scaled = scaler.fit_transform(train_X)

# Step 4: Apply PCA (Principal Component Analysis) to retain 95% variance
pca = PCA(n_components=0.95)
train_X_pca = pca.fit_transform(train_X_scaled)

# Step 5: Define the XGBoost model parameters
xgb_params = {
    'eta': 0.05,
    'max_depth': 6,
    'subsample': 0.7,
    'colsample_bytree': 0.7,
    'objective': 'reg:squarederror',  # Use 'reg:squarederror' for regression tasks
    'silent': 1
}

# Custom R-squared evaluation function for XGBoost
def xgb_r2_score(preds, dtrain):
    labels = dtrain.get_label()
    return 'r2', r2_score(labels, preds)

# Step 6: Train the model without PCA (using original features)
dtrain_original = xgb.DMatrix(train_X, train_y)
model_original = xgb.train(dict(xgb_params, silent=0), dtrain_original, num_boost_round=100, feval=xgb_r2_score, maximize=True)

# Step 7: Evaluate the model performance without PCA (R-squared score)
train_preds_original = model_original.predict(dtrain_original)
train_r2_original = r2_score(train_y, train_preds_original)
print(f"Training R-squared score without PCA: {train_r2_original:.4f}")

# Step 8: Train the model with PCA (using PCA-transformed features)
dtrain_pca = xgb.DMatrix(train_X_pca, train_y)
model_pca = xgb.train(dict(xgb_params, silent=0), dtrain_pca, num_boost_round=100, feval=xgb_r2_score, maximize=True)

# Step 9: Evaluate the model performance with PCA (R-squared score)
train_preds_pca = model_pca.predict(dtrain_pca)
train_r2_pca = r2_score(train_y, train_preds_pca)
print(f"Training R-squared score with PCA: {train_r2_pca:.4f}")

# Step 10: Map PCA components to original feature names
# pca.components_ gives the principal components (loadings)
pca_loadings = pca.components_

# Create a DataFrame to view the feature contributions to each component
pca_df = pd.DataFrame(pca_loadings.T, columns=[f'PC{i+1}' for i in range(pca_loadings.shape[0])], index=train_X.columns)

# Print the top contributing features for each PCA component
print("Top contributing features for each PCA component:")
for i in range(pca_loadings.shape[0]):
    print(f"\nPrincipal Component {i+1}:")
    print(pca_df.iloc[:, i].sort_values(ascending=False).head(10))  # Top 10 features for each component

# Step 11: Plot feature importance based on PCA components
# To plot the PCA components with respect to original features, we'll show the contribution (absolute value of loadings)
feature_importance = np.abs(pca_loadings).sum(axis=0)
sorted_idx = np.argsort(feature_importance)[::-1]  # Sort by importance

# Plot the top 10 features based on PCA loadings
plt.figure(figsize=(12, 6))
plt.bar(range(10), feature_importance[sorted_idx[:10]])
plt.xticks(range(10), train_X.columns[sorted_idx[:10]], rotation=90)
plt.title('Top 10 Features Based on PCA Loadings')
plt.xlabel('Original Features')
plt.ylabel('Contribution to Principal Components')
plt.show()
